{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Enable eager execution\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoFeatureExtractor, TFAutoModelForImageClassification, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data_dir = 'C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Car_Detection_Chat\\\\testando'\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_object = Path('C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Car_Detection_Chat\\\\testando')  \n",
    "\n",
    "\n",
    "# Fetch all images from the dataset\n",
    "all_images = list(data_dir_object.glob('**/*.jpg'))  # assuming all images are jpg. You can adjust the pattern accordingly.\n",
    "\n",
    "# Sample 9 random images\n",
    "random_images = random.sample(all_images, 9)\n",
    "\n",
    "# Display the images\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for ax, img_path in zip(axes.ravel(), random_images):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(img_path.name, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Verify problems wtht the files\n",
    "\n",
    "# for root, dirs, files in os.walk(data_dir):\n",
    "#     for file in files:\n",
    "#         if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             try:\n",
    "#                 with Image.open(file_path) as img:\n",
    "#                     img.verify()\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Problem with file: {file_path}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHECK ENCODING\n",
    "# def check_utf8_encoding(data_dir):\n",
    "#     for root, dirs, files in os.walk(data_dir):\n",
    "#         # # Check directory names\n",
    "#         # for dir_name in dirs:\n",
    "#         #     try:\n",
    "#         #         dir_name.decode('utf-8')\n",
    "#         #     except UnicodeDecodeError:\n",
    "#         #         print(f\"Non-UTF-8 directory name detected: {os.path.join(root, dir_name)}\")\n",
    "        \n",
    "#         # Check file names\n",
    "#         for file_name in files:\n",
    "#             try:\n",
    "#                 file_name.encode('utf-8').decode('utf-8')\n",
    "#             except UnicodeDecodeError:\n",
    "#                 print(f\"Non-UTF-8 file name detected: {os.path.join(root, file_name)}\")\n",
    "\n",
    "# data_dir = 'C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Car_Detection_Chat\\\\images_backup'\n",
    "# check_utf8_encoding(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img, label):\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "train_ds = train_ds.map(normalize_image)\n",
    "val_ds = val_ds.map(normalize_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, sample_label = next(iter(train_ds.take(1)))\n",
    "print(\"Min Value:\", tf.reduce_min(sample_img).numpy())\n",
    "print(\"Max Value:\", tf.reduce_max(sample_img).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU Memory Growth\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ViT model and feature extractor\n",
    "extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "def prepare_data(images, labels):\n",
    "    images_np = images.numpy()\n",
    "    # Optional: Clip values just to be sure they're between 0 and 1\n",
    "    images_np = np.clip(images_np, 0, 1)\n",
    "    \n",
    "    inputs = extractor(images_np, return_tensors=\"tf\")\n",
    "    return inputs[\"pixel_values\"], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),  # Rotates the image up to a fraction of 0.1 of 360 degrees\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),      # Randomly zooms the image up to 20%\n",
    "    layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),  # Randomly translates the image up to 10% horizontally and vertically\n",
    "    # Add more augmentation layers if needed\n",
    "])\n",
    "\n",
    "# Apply augmentation to the training dataset\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: tf.py_function(prepare_data, [x, y], [tf.float32, tf.int32]))\n",
    "val_ds = val_ds.map(lambda x, y: tf.py_function(prepare_data, [x, y], [tf.float32, tf.int32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
